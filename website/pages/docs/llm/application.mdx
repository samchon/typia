---
title: Guide Documents > Large Language Model > application() function
---
import { Callout, Tabs } from 'nextra/components'

## `application()` function
<Tabs items={[
    <code>typia</code>,
    <code>ILlmApplication</code>,
    <code>ILlmFunction</code>,
    <code>ILlmSchema</code>,
  ]}>
  <Tabs.Tab>
```typescript filename="typia" showLineNumbers {2-9}
export namespace llm {
  // LLM FUNCTION CALLING APPLICATION SCHEMA
  export function application<
    App extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    options?: Partial<Pick<ILlmApplication.IOptions<Model>, "separate">>,
  ): ILlmApplication<Model>;

  // +VALIDATE FUNCTION EMBEDDED
  export function applicationOfValidate<
    App extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    options?: Partial<Pick<ILlmApplicationOfValidate.IOptions<Model>, "separate">>,
  ): ILlmApplicationOfValidate<Model>;

  // STRUCTURED OUTPUT
  export function parameters<
    Parameters extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(): ILlmSchema.ModelParameters[Model];

  // TYPE SCHEMA
  export function schema<
    T,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    ...$defs: Extract<
      ILlmSchema.ModelSchema[Model],
      { $ref: string }
    > extends never
      ? []
      : [Record<string, ILlmSchema.ModelSchema[Model]>]
  ): ILlmSchema.ModelSchema[Model];
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="@samchon/openapi" showLineNumbers
import { IGeminiSchema } from "./IGeminiSchema";
import { ILlmFunction } from "./ILlmFunction";
import { ILlmSchema } from "./ILlmSchema";

/**
 * Application of LLM function calling.
 *
 * `ILlmApplication` is a data structure representing a collection of
 * {@link ILlmFunction LLM function calling schemas}, composed from a native
 * TypeScript class (or interface) type by the `typia.llm.application<App, Model>()`
 * function.
 *
 * Also, there can be some parameters (or their nested properties) which must be
 * composed by Human, not by LLM. File uploading feature or some sensitive information
 * like security keys (password) are the examples. In that case, you can separate the
 * function parameters to both LLM and human sides by configuring the
 * {@link ILlmApplication.IOptions.separate} property. The separated parameters are
 * assigned to the {@link ILlmFunction.separated} property.
 *
 * For reference, when both LLM and Human filled parameter values to call, you can
 * merge them by calling the {@link HttpLlm.mergeParameters} function. In other words,
 * if you've configured the {@link ILlmApplication.IOptions.separate} property, you
 * have to merge the separated parameters before the function call execution.
 *
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @author Jeongho Nam - https://github.com/samchon
 */
export interface ILlmApplication<Model extends ILlmSchema.Model> {
  /**
   * Model of the LLM.
   */
  model: Model;

  /**
   * List of function metadata.
   *
   * List of function metadata that can be used for the LLM function call.
   */
  functions: ILlmFunction<Model>[];

  /**
   * Configuration for the application.
   */
  options: ILlmApplication.IOptions<Model>;
}
export namespace ILlmApplication {
  /**
   * Options for application composition.
   */
  export type IOptions<Model extends ILlmSchema.Model> = {
    /**
     * Separator function for the parameters.
     *
     * When composing parameter arguments through LLM function call,
     * there can be a case that some parameters must be composed by human,
     * or LLM cannot understand the parameter.
     *
     * For example, if the parameter type has configured
     * {@link IGeminiSchema.IString.contentMediaType} which indicates file
     * uploading, it must be composed by human, not by LLM
     * (Large Language Model).
     *
     * In that case, if you configure this property with a function that
     * predicating whether the schema value must be composed by human or
     * not, the parameters would be separated into two parts.
     *
     * - {@link ILlmFunction.separated.llm}
     * - {@link ILlmFunction.separated.human}
     *
     * When writing the function, note that returning value `true` means
     * to be a human composing the value, and `false` means to LLM
     * composing the value. Also, when predicating the schema, it would
     * better to utilize the {@link GeminiTypeChecker} like features.
     *
     * @param schema Schema to be separated.
     * @returns Whether the schema value must be composed by human or not.
     * @default null
     */
    separate: null | ((schema: ILlmSchema.ModelSchema[Model]) => boolean);
  } & ILlmSchema.ModelConfig[Model];
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="@samchon/openapi" showLineNumbers
import { ILlmSchema } from "./ILlmSchema";

/**
 * LLM function metadata.
 *
 * `ILlmFunction` is an interface representing a function metadata,
 * which has been used for the LLM (Language Large Model) function
 * calling. Also, it's a function structure containing the function
 * {@link name}, {@link parameters} and {@link output return type}.
 *
 * If you provide this `ILlmFunction` data to the LLM provider like "OpenAI",
 * the "OpenAI" will compose a function arguments by analyzing conversations
 * with the user. With the LLM composed arguments, you can execute the function
 * and get the result.
 *
 * By the way, do not ensure that LLM will always provide the correct
 * arguments. The LLM of present age is not perfect, so that you would
 * better to validate the arguments before executing the function.
 * I recommend you to validate the arguments before execution by using
 * [`typia`](https://github.com/samchon/typia) library.
 *
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @author Jeongho Nam - https://github.com/samchon
 */
export interface ILlmFunction<Model extends ILlmSchema.Model> {
  /**
   * Representative name of the function.
   */
  name: string;

  /**
   * List of parameter types.
   */
  parameters: ILlmSchema.ModelParameters[Model];

  /**
   * Collection of separated parameters.
   */
  separated?: ILlmFunction.ISeparated<Model>;

  /**
   * Expected return type.
   *
   * If the function returns nothing (`void`), the `output` value would
   * be `undefined`.
   */
  output?: ILlmSchema.ModelSchema[Model];

  /**
   * Whether the function schema types are strict or not.
   *
   * Newly added specification to "OpenAI" at 2024-08-07.
   *
   * @reference https://openai.com/index/introducing-structured-outputs-in-the-api/
   */
  strict: true;

  /**
   * Description of the function.
   *
   * For reference, the `description` is very important property to teach
   * the purpose of the function to the LLM (Language Large Model), and
   * LLM actually determines which function to call by the description.
   *
   * Also, when the LLM conversates with the user, the `description` is
   * used to explain the function to the user. Therefore, the `description`
   * property has the highest priority, and you have to consider it.
   */
  description?: string | undefined;

  /**
   * Whether the function is deprecated or not.
   *
   * If the `deprecated` is `true`, the function is not recommended to use.
   *
   * LLM (Large Language Model) may not use the deprecated function.
   */
  deprecated?: boolean | undefined;

  /**
   * Category tags for the function.
   *
   * You can fill this property by the `@tag ${name}` comment tag.
   */
  tags?: string[];
}
export namespace ILlmFunction {
  /**
   * Collection of separated parameters.
   */
  export interface ISeparated<Model extends ILlmSchema.Model> {
    /**
     * Parameters that would be composed by the LLM.
     */
    llm: ILlmSchema.ModelParameters[Model] | null;

    /**
     * Parameters that would be composed by the human.
     */
    human: ILlmSchema.ModelParameters[Model] | null;
  }
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="@samchon/openapi" showLineNumbers
import { IChatGptSchema } from "./IChatGptSchema";
import { IClaudeSchema } from "./IClaudeSchema";
import { IGeminiSchema } from "./IGeminiSchema";
import { ILlamaSchema } from "./ILlamaSchema";
import { ILlmSchemaV3 } from "./ILlmSchemaV3";
import { ILlmSchemaV3_1 } from "./ILlmSchemaV3_1";

/**
 * The schemas for the LLM function calling.
 *
 * `ILlmSchema` is an union type collecting every the schemas for the
 * LLM function calling.
 *
 * Select a proper schema type according to the LLM provider you're using.
 *
 * @template Model Name of the target LLM model
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @reference https://platform.openai.com/docs/guides/structured-outputs
 * @author Jeongho Nam - https://github.com/samchon
 */
export type ILlmSchema<Model extends ILlmSchema.Model = ILlmSchema.Model> =
  ILlmSchema.ModelSchema[Model];

export namespace ILlmSchema {
  export type Model = "chatgpt" | "claude" | "gemini" | "llama" | "3.0" | "3.1";
  export interface ModelConfig {
    chatgpt: IChatGptSchema.IConfig;
    claude: IClaudeSchema.IConfig;
    gemini: IGeminiSchema.IConfig;
    llama: ILlamaSchema.IConfig;
    "3.0": ILlmSchemaV3.IConfig;
    "3.1": ILlmSchemaV3_1.IConfig;
  }
  export interface ModelParameters {
    chatgpt: IChatGptSchema.IParameters;
    claude: IClaudeSchema.IParameters;
    gemini: IGeminiSchema.IParameters;
    llama: ILlamaSchema.IParameters;
    "3.0": ILlmSchemaV3.IParameters;
    "3.1": ILlmSchemaV3_1.IParameters;
  }
  export interface ModelSchema {
    chatgpt: IChatGptSchema;
    claude: IClaudeSchema;
    gemini: IGeminiSchema;
    llama: ILlamaSchema;
    "3.0": ILlmSchemaV3;
    "3.1": ILlmSchemaV3_1;
  }

  /**
   * Type of function parameters.
   *
   * `ILlmSchema.IParameters` is a type defining a function's pamameters
   * as a keyworded object type.
   *
   * It also can be utilized for the structured output metadata.
   *
   * @reference https://platform.openai.com/docs/guides/structured-outputs
   */
  export type IParameters<Model extends ILlmSchema.Model = ILlmSchema.Model> =
    ILlmSchema.ModelParameters[Model];

  /**
   * Configuration for the LLM schema composition.
   */
  export type IConfig<Model extends ILlmSchema.Model = ILlmSchema.Model> =
    ILlmSchema.ModelConfig[Model];
}
```
  </Tabs.Tab>
</Tabs>

LLM function calling application schema from a native TypeScript class or interface type.

`typia.llm.application<App, Model>()` is a function composing LLM (Large Language Model) calling application schema from a native TypeScript class or interface type. The function returns an `ILlmApplication` instance, which is a data structure representing a collection of LLM function calling schemas. 

If you put LLM function schema instances registered in the `ILlmApplication.functions` to the LLM provider like `OpenAI ChatGPT`, the LLM will select a proper function to call with parameter values of the target function in the conversations with the user. This is the "LLM Function Calling".

You can specify the LLM provide model by the second `Model` template argument. It's because detailed specification of the function schema is different by the LLM provider model. Here is the list of LLM schema definitions of each model. Determine one of them carefully reading the LLM schema definitions.

If you've determined, let's make A.I. Chatbot super-easily with `typia.llm.application<App, Model>()` function.

  - Supported schemas
    - [`IChatGptSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IChatGptSchema.ts): OpenAI ChatGPT
    - [`IClaudeSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IClaudeSchema.ts): Anthropic Claude
    - [`IGeminiSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IGeminiSchema.ts): Google Gemini
    - [`ILlamaSchema`](https://github.com/samchon/openapi/blob/master/src/structures/ILlamaSchema.ts): Meta Llama
  - Midldle layer schemas
    - [`ILlmSchemaV3`](https://github.com/samchon/openapi/blob/master/src/structures/ILlmSchemaV3.ts): middle layer based on OpenAPI v3.0 specification
    - [`ILlmSchemaV3_1`](https://github.com/samchon/openapi/blob/master/src/structures/ILlmSchemaV3_1.ts): middle layer based on OpenAPI v3.1 specification

<Callout type="info">
**LLM Function Calling** and **Structured Output**

LLM selects proper function and fill arguments.

In nowadays, most LLM (Large Language Model) like OpenAI are supporting "function calling" feature. The "LLM function calling" means that LLM automatically selects a proper function and fills parameter values from conversation with the user (may by chatting text).

Structured output is another feature of LLM. The "structured output" means that LLM automatically transforms the output conversation into a structured data format like JSON.

- https://platform.openai.com/docs/guides/function-calling
- https://platform.openai.com/docs/guides/structured-outputs
</Callout>




## `applicationOfValidate()`
<Tabs items={[
    <code>typia</code>,
    <code>ILlmApplication</code>,
    <code>ILlmFunction</code>,
    <code>ILlmSchema</code>,
    <code>ILlmApplicationOfValidate</code>,
  ]}>
  <Tabs.Tab>
```typescript filename="typia" showLineNumbers {11-18}
export namespace llm {
  // LLM FUNCTION CALLING APPLICATION SCHEMA
  export function application<
    App extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    options?: Partial<Pick<ILlmApplication.IOptions<Model>, "separate">>,
  ): ILlmApplication<Model>;

  // +VALIDATE FUNCTION EMBEDDED
  export function applicationOfValidate<
    App extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    options?: Partial<Pick<ILlmApplicationOfValidate.IOptions<Model>, "separate">>,
  ): ILlmApplicationOfValidate<Model>;

  // STRUCTURED OUTPUT
  export function parameters<
    Parameters extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(): ILlmSchema.ModelParameters[Model];

  // TYPE SCHEMA
  export function schema<
    T,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    ...$defs: Extract<
      ILlmSchema.ModelSchema[Model],
      { $ref: string }
    > extends never
      ? []
      : [Record<string, ILlmSchema.ModelSchema[Model]>]
  ): ILlmSchema.ModelSchema[Model];
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="@samchon/openapi" showLineNumbers
import { IGeminiSchema } from "./IGeminiSchema";
import { ILlmFunction } from "./ILlmFunction";
import { ILlmSchema } from "./ILlmSchema";

/**
 * Application of LLM function calling.
 *
 * `ILlmApplication` is a data structure representing a collection of
 * {@link ILlmFunction LLM function calling schemas}, composed from a native
 * TypeScript class (or interface) type by the `typia.llm.application<App, Model>()`
 * function.
 *
 * Also, there can be some parameters (or their nested properties) which must be
 * composed by Human, not by LLM. File uploading feature or some sensitive information
 * like security keys (password) are the examples. In that case, you can separate the
 * function parameters to both LLM and human sides by configuring the
 * {@link ILlmApplication.IOptions.separate} property. The separated parameters are
 * assigned to the {@link ILlmFunction.separated} property.
 *
 * For reference, when both LLM and Human filled parameter values to call, you can
 * merge them by calling the {@link HttpLlm.mergeParameters} function. In other words,
 * if you've configured the {@link ILlmApplication.IOptions.separate} property, you
 * have to merge the separated parameters before the function call execution.
 *
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @author Jeongho Nam - https://github.com/samchon
 */
export interface ILlmApplication<Model extends ILlmSchema.Model> {
  /**
   * Model of the LLM.
   */
  model: Model;

  /**
   * List of function metadata.
   *
   * List of function metadata that can be used for the LLM function call.
   */
  functions: ILlmFunction<Model>[];

  /**
   * Configuration for the application.
   */
  options: ILlmApplication.IOptions<Model>;
}
export namespace ILlmApplication {
  /**
   * Options for application composition.
   */
  export type IOptions<Model extends ILlmSchema.Model> = {
    /**
     * Separator function for the parameters.
     *
     * When composing parameter arguments through LLM function call,
     * there can be a case that some parameters must be composed by human,
     * or LLM cannot understand the parameter.
     *
     * For example, if the parameter type has configured
     * {@link IGeminiSchema.IString.contentMediaType} which indicates file
     * uploading, it must be composed by human, not by LLM
     * (Large Language Model).
     *
     * In that case, if you configure this property with a function that
     * predicating whether the schema value must be composed by human or
     * not, the parameters would be separated into two parts.
     *
     * - {@link ILlmFunction.separated.llm}
     * - {@link ILlmFunction.separated.human}
     *
     * When writing the function, note that returning value `true` means
     * to be a human composing the value, and `false` means to LLM
     * composing the value. Also, when predicating the schema, it would
     * better to utilize the {@link GeminiTypeChecker} like features.
     *
     * @param schema Schema to be separated.
     * @returns Whether the schema value must be composed by human or not.
     * @default null
     */
    separate: null | ((schema: ILlmSchema.ModelSchema[Model]) => boolean);
  } & ILlmSchema.ModelConfig[Model];
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="@samchon/openapi" showLineNumbers
import { ILlmSchema } from "./ILlmSchema";

/**
 * LLM function metadata.
 *
 * `ILlmFunction` is an interface representing a function metadata,
 * which has been used for the LLM (Language Large Model) function
 * calling. Also, it's a function structure containing the function
 * {@link name}, {@link parameters} and {@link output return type}.
 *
 * If you provide this `ILlmFunction` data to the LLM provider like "OpenAI",
 * the "OpenAI" will compose a function arguments by analyzing conversations
 * with the user. With the LLM composed arguments, you can execute the function
 * and get the result.
 *
 * By the way, do not ensure that LLM will always provide the correct
 * arguments. The LLM of present age is not perfect, so that you would
 * better to validate the arguments before executing the function.
 * I recommend you to validate the arguments before execution by using
 * [`typia`](https://github.com/samchon/typia) library.
 *
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @author Jeongho Nam - https://github.com/samchon
 */
export interface ILlmFunction<Model extends ILlmSchema.Model> {
  /**
   * Representative name of the function.
   */
  name: string;

  /**
   * List of parameter types.
   */
  parameters: ILlmSchema.ModelParameters[Model];

  /**
   * Collection of separated parameters.
   */
  separated?: ILlmFunction.ISeparated<Model>;

  /**
   * Expected return type.
   *
   * If the function returns nothing (`void`), the `output` value would
   * be `undefined`.
   */
  output?: ILlmSchema.ModelSchema[Model];

  /**
   * Whether the function schema types are strict or not.
   *
   * Newly added specification to "OpenAI" at 2024-08-07.
   *
   * @reference https://openai.com/index/introducing-structured-outputs-in-the-api/
   */
  strict: true;

  /**
   * Description of the function.
   *
   * For reference, the `description` is very important property to teach
   * the purpose of the function to the LLM (Language Large Model), and
   * LLM actually determines which function to call by the description.
   *
   * Also, when the LLM conversates with the user, the `description` is
   * used to explain the function to the user. Therefore, the `description`
   * property has the highest priority, and you have to consider it.
   */
  description?: string | undefined;

  /**
   * Whether the function is deprecated or not.
   *
   * If the `deprecated` is `true`, the function is not recommended to use.
   *
   * LLM (Large Language Model) may not use the deprecated function.
   */
  deprecated?: boolean | undefined;

  /**
   * Category tags for the function.
   *
   * You can fill this property by the `@tag ${name}` comment tag.
   */
  tags?: string[];
}
export namespace ILlmFunction {
  /**
   * Collection of separated parameters.
   */
  export interface ISeparated<Model extends ILlmSchema.Model> {
    /**
     * Parameters that would be composed by the LLM.
     */
    llm: ILlmSchema.ModelParameters[Model] | null;

    /**
     * Parameters that would be composed by the human.
     */
    human: ILlmSchema.ModelParameters[Model] | null;
  }
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="@samchon/openapi" showLineNumbers
import { IChatGptSchema } from "./IChatGptSchema";
import { IClaudeSchema } from "./IClaudeSchema";
import { IGeminiSchema } from "./IGeminiSchema";
import { ILlamaSchema } from "./ILlamaSchema";
import { ILlmSchemaV3 } from "./ILlmSchemaV3";
import { ILlmSchemaV3_1 } from "./ILlmSchemaV3_1";

/**
 * The schemas for the LLM function calling.
 *
 * `ILlmSchema` is an union type collecting every the schemas for the
 * LLM function calling.
 *
 * Select a proper schema type according to the LLM provider you're using.
 *
 * @template Model Name of the target LLM model
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @reference https://platform.openai.com/docs/guides/structured-outputs
 * @author Jeongho Nam - https://github.com/samchon
 */
export type ILlmSchema<Model extends ILlmSchema.Model = ILlmSchema.Model> =
  ILlmSchema.ModelSchema[Model];

export namespace ILlmSchema {
  export type Model = "chatgpt" | "claude" | "gemini" | "llama" | "3.0" | "3.1";
  export interface ModelConfig {
    chatgpt: IChatGptSchema.IConfig;
    claude: IClaudeSchema.IConfig;
    gemini: IGeminiSchema.IConfig;
    llama: ILlamaSchema.IConfig;
    "3.0": ILlmSchemaV3.IConfig;
    "3.1": ILlmSchemaV3_1.IConfig;
  }
  export interface ModelParameters {
    chatgpt: IChatGptSchema.IParameters;
    claude: IClaudeSchema.IParameters;
    gemini: IGeminiSchema.IParameters;
    llama: ILlamaSchema.IParameters;
    "3.0": ILlmSchemaV3.IParameters;
    "3.1": ILlmSchemaV3_1.IParameters;
  }
  export interface ModelSchema {
    chatgpt: IChatGptSchema;
    claude: IClaudeSchema;
    gemini: IGeminiSchema;
    llama: ILlamaSchema;
    "3.0": ILlmSchemaV3;
    "3.1": ILlmSchemaV3_1;
  }

  /**
   * Type of function parameters.
   *
   * `ILlmSchema.IParameters` is a type defining a function's pamameters
   * as a keyworded object type.
   *
   * It also can be utilized for the structured output metadata.
   *
   * @reference https://platform.openai.com/docs/guides/structured-outputs
   */
  export type IParameters<Model extends ILlmSchema.Model = ILlmSchema.Model> =
    ILlmSchema.ModelParameters[Model];

  /**
   * Configuration for the LLM schema composition.
   */
  export type IConfig<Model extends ILlmSchema.Model = ILlmSchema.Model> =
    ILlmSchema.ModelConfig[Model];
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```typescript filename="typia" showLineNumbers
import { ILlmApplication, ILlmFunction, ILlmSchema } from "@samchon/openapi";

import { IValidation } from "../../IValidation";

/**
 * Application of LLM function calling with validators.
 *
 * `ILlmApplication` is a data structure representing a collection of
 * {@link ILlmFunctionOfValidate LLM function calling schemas}, composed from a native
 * TypeScript class (or interface) type by the `typia.llm.applicationOfValidate<App, Model>()`
 * function.
 *
 * If you put the returned {@link ILlmApplicationOfValidate.functions} objects to the
 * LLM provider like [OpenAI (ChatGPT)](https://openai.com/), the LLM will automatically
 * select the proper function and fill its arguments from the conversation
 * (maybe chatting text) with user (human). This is the concept of the LLM function calling.
 *
 * Additionally, the LLM function calling sometimes take a mistake that composing wrong typed
 * {@link ILlmFunctionOfValidate.parameters}. In that case, deliver return value of the
 * {@link ILlmFunctionOfValidate.validate} function, then LLM provider will correct the
 * parameters at the next conversation. The {@link ILlmFunctionOfValidate.validate} function
 * is a validator function reporting the detailed information about the wrong typed parameters.
 *
 * By the way, there can be some parameters (or their nested properties) which must be
 * composed by Human, not by LLM. File uploading feature or some sensitive information
 * like security keys (password) are the examples. In that case, you can separate the
 * function parameters to both LLM and human sides by configuring the
 * {@link ILlmApplication.IOptions.separate} property. The separated parameters are
 * assigned to the {@link ILlmFunction.separated} property.
 *
 * For reference, when both LLM and Human filled parameter values to call, you can
 * merge them by calling the {@link HttpLlm.mergeParameters} function. In other words,
 * if you've configured the {@link ILlmApplication.IOptions.separate} property, you
 * have to merge the separated parameters before the function call execution.
 *
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @author Jeongho Nam - https://github.com/samchon
 */
export interface ILlmApplicationOfValidate<Model extends ILlmSchema.Model>
  extends ILlmApplication<Model> {
  /**
   * List of function metadata.
   *
   * List of function metadata that can be used for the LLM function call.
   *
   * Also, every functions have their own parameters validator
   * {@link ILlmFunctionOfValidate.validate}. If the LLM function calling composes wrong
   * typed parameters, then deliver return value of it, then LLM will correct parameters
   * at the next conversation.
   */
  functions: ILlmFunctionOfValidate<Model>[];
}
export namespace ILlmApplicationOfValidate {
  export import IOptions = ILlmApplication.IOptions;
}

/**
 * LLM function metadata with validator.
 *
 * `ILlmFunctionOfValidate` is an interface representing a function metadata,
 * which has been used for the LLM (Language Large Model) function
 * calling. Also, it's a function structure containing the function
 * {@link name}, {@link parameters} and {@link output return type}.
 *
 * If you provide this `ILlmFunctionOfValidate` data to the LLM provider like "OpenAI",
 * the "OpenAI" will compose a function arguments by analyzing conversations
 * with the user. With the LLM composed arguments, you can execute the function
 * and get the result.
 *
 * If the LLM function calling take s a mistake that composing wrong typed
 * {@link parameters}, you can correct the parameters by delivering the return
 * value of the {@link validate} function. The {@link validate} function is a
 * validator function reporting the detailed information about the wrong typed
 * {@link parameters}.
 *
 * By the way, do not ensure that LLM will always provide the correct arguments.
 * The LLM of present age is not perfect, and sometimes takes a mistake that composing
 * wrong typed {@link parameters}. In that case, you can correc the parameters by
 * delivering the return value of the {@link validate} function. The {@link validate}
 * function reports the detailed information about the wrong typed {@link parameters},
 *
 * @reference https://platform.openai.com/docs/guides/function-calling
 * @author Jeongho Nam - https://github.com/samchon
 */
export interface ILlmFunctionOfValidate<Model extends ILlmSchema.Model>
  extends ILlmFunction<Model> {
  validate(props: object): IValidation<unknown>;
}
export namespace ILlmFunctionOfValidate {
  export import ISeparated = ILlmFunction.ISeparated;
}
```
  </Tabs.Tab>
</Tabs>

LLM function calling application schema with validators.

`typia.llm.applicationOfValidate<App, Model>()` is a function combined `typia.llm.application<App, Model>()` and [`typia.validate<T>()`](../validators/validate) functions. Every function schema instances registered in the `ILlmApplicationOfValidate.functions` have their own parameters validator `ILlmFunctionOfValidate.validate()`, and they will report the detailed information about the wrong typed parameters.

This validator function is useful when implementing the actual LLM function calling feature. It's because LLM function calling sometimes takes a mistake that composing wrong typed arguments. In that case, you can correct the arguments by delivering the return value of the `ILlmFunctionOfValidate.validate()` function to the LLM provider. Then, the LLM provider will correct the arguments at the next function calling.

Here is an actual program code correcting the OpenAI (ChatGPT) function calling. 

Note that, if you are developing an A.I. chatbot project, such validation feedback strategy is essential for both LLM function calling and structured output features. Tends to my experiments, even though the LLM makes a wrong typed structured data, it always be corrected just by only one validation feedback step.

```typescript filename="ChatGptFunctionCaller.ts" showLineNumbers {22, 33-43, 72} copy
import OpenAI from "openai";
import typia, { ILlmFunctionOfValidate, IValidation } from "typia";

import { ILlmMessage } from "../structures/ILlmMessage";

export namespace ChatGptFunctionCaller {
  export interface IProps {
    function: ILlmFunctionOfValidate<"chatgpt">;
    messages: ILlmMessage[];
  }

  export const test = async (props: IProps): Promise<void> => {
    let result: IValidation<any> | undefined = undefined;
    for (let i: number = 0; i < 3; ++i) {
      if (result && result.success === true) break;
      result = await step(props, result);
    }
  };

  const step = async (
    props: IProps,
    previous?: IValidation.IFailure | undefined,
  ): Promise<IValidation<any>> => {
    const client: OpenAI = new OpenAI({
      apiKey: "YOUR-SECRET-KEY",
    });
    const completion: OpenAI.ChatCompletion =
      await client.chat.completions.create({
        model: "gpt-4o",
        messages: previous
          ? [
              ...props.messages.slice(0, -1),
              {
                role: "assistant",
                content: [
                  "You A.I. assistant has composed wrong typed arguments.",
                  "",
                  "Here is the detailed list of type errors. Review and correct them at the next function calling.",
                  "",
                  "```json",
                  JSON.stringify(previous.errors, null, 2),
                  "```",
                ].join("\n"),
              } satisfies OpenAI.ChatCompletionMessageParam,
              ...props.messages.slice(-1),
            ]
          : props.messages,
        tools: [
          {
            type: "function",
            function: {
              name: props.function.name,
              description: props.function.description,
              parameters: props.function.parameters as Record<string, any>,
            },
          },
        ],
        tool_choice: "required",
        parallel_tool_calls: false,
      });

    const toolCalls: OpenAI.ChatCompletionMessageToolCall[] = completion.choices
      .map((c) => c.message.tool_calls ?? [])
      .flat();
    if (toolCalls.length === 0)
      throw new Error("ChatGPT has not called any function.");

    const results: IValidation<any>[] = toolCalls.map((call) => {
      const { input } = typia.assert<{ input: any }>(
        JSON.parse(call.function.arguments),
      );
      return props.function.validate(input);
    });
    return results.find((r) => r.success === true) ?? results[0];
  };
}
```




## Parameters' Separation
Parameter values from both LLM and Human sides.

When composing parameter arguments through the LLM (Large Language Model) function calling, there can be a case that some parameters (or nested properties) must be composed not by LLM, but by Human. File uploading feature, or sensitive information like secret key (password) cases are the representative examples.

In that case, you can configure the LLM function calling schemas to exclude such Human side parameters (or nested properties) by `ILlmApplication.options.separate` property. Instead, you have to merge both Human and LLM composed parameters into one by calling the `HttpLlm.mergeParameters()` before the LLM function call execution.

Here is the example separating the parameter schemas.

<Tabs items={[
    "TypeScript Source Code",
    "LLM Function Calling Application Schema",
  ]}>
  <Tabs.Tab>
```typescript filename="example/src/llm.application.separate.ts" showLineNumbers {4-7}
import { ILlmApplication } from "@samchon/openapi";
import typia, { tags } from "typia";

const app: ILlmApplication<"claude"> = typia.llm.application<
  BbsArticleController,
  "claude"
>({
  separate: (schema: ILlmSchema<"claude">) =>
    ClaudeTypeChecker.isString(schema) && schema.contentMediaType !== undefined,
});

console.log(app);

interface BbsArticleController {
  /**
   * Create a new article.
   *
   * Writes a new article and archives it into the DB.
   *
   * @param props Properties of create function
   * @returns Newly created article
   */
  create(props: {
    /**
     * Information of the article to create
     */
    input: IBbsArticle.ICreate;
  }): Promise<IBbsArticle>;

  /**
   * Update an article.
   *
   * Updates an article with new content.
   *
   * @param props Properties of update function
   * @param input New content to update
   */
  update(props: {
    /**
     * Target article's {@link IBbsArticle.id}.
     */
    id: string & tags.Format<"uuid">;

    /**
     * New content to update.
     */
    input: IBbsArticle.IUpdate;
  }): Promise<void>;

  /**
   * Erase an article.
   *
   * Erases an article from the DB.
   *
   * @param props Properties of erase function
   */
  erase(props: {
    /**
     * Target article's {@link IBbsArticle.id}.
     */
    id: string & tags.Format<"uuid">;
  }): Promise<void>;
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```json filename="example/schemas/llm.application.separate.json" showLineNumbers {41-91}
{
  "model": "claude",
  "options": {
    "reference": false
  },
  "functions": [
    {
      "name": "create",
      "parameters": {
        "type": "object",
        "properties": {
          "input": {
            "description": "Information of the article to create.\n\n------------------------------\n\nDescription of the current {@link IBbsArticle.ICreate} type:\n\n> Information of the article to create.\n\n------------------------------\n\nDescription of the parent {@link IBbsArticle} type:\n\n> Article entity.\n> \n> `IBbsArticle` is an entity representing an article in the BBS (Bulletin Board System).",
            "type": "object",
            "properties": {
              "title": {
                "title": "Title of the article",
                "description": "Title of the article.\n\nRepresentative title of the article.",
                "type": "string"
              },
              "body": {
                "title": "Content body",
                "description": "Content body.\n\nContent body of the article writtn in the markdown format.",
                "type": "string"
              },
              "thumbnail": {
                "title": "Thumbnail image URI",
                "description": "Thumbnail image URI.\n\nThumbnail image URI which can represent the article.\n\nIf configured as `null`, it means that no thumbnail image in the article.",
                "oneOf": [
                  {
                    "type": "null"
                  },
                  {
                    "type": "string",
                    "format": "uri",
                    "contentMediaType": "image/*"
                  }
                ]
              }
            },
            "required": [
              "title",
              "body",
              "thumbnail"
            ]
          }
        },
        "required": [
          "input"
        ],
        "additionalProperties": false,
        "$defs": {}
      },
      "output": {
        "description": "Article entity.\n\n`IBbsArticle` is an entity representing an article in the BBS (Bulletin Board System).",
        "type": "object",
        "properties": {
          "id": {
            "title": "Primary Key",
            "description": "Primary Key.",
            "type": "string",
            "format": "uuid"
          },
          "created_at": {
            "title": "Creation time of the article",
            "description": "Creation time of the article.",
            "type": "string",
            "format": "date-time"
          },
          "updated_at": {
            "title": "Last updated time of the article",
            "description": "Last updated time of the article.",
            "type": "string",
            "format": "date-time"
          },
          "title": {
            "title": "Title of the article",
            "description": "Title of the article.\n\nRepresentative title of the article.",
            "type": "string"
          },
          "body": {
            "title": "Content body",
            "description": "Content body.\n\nContent body of the article writtn in the markdown format.",
            "type": "string"
          },
          "thumbnail": {
            "title": "Thumbnail image URI",
            "description": "Thumbnail image URI.\n\nThumbnail image URI which can represent the article.\n\nIf configured as `null`, it means that no thumbnail image in the article.",
            "oneOf": [
              {
                "type": "null"
              },
              {
                "type": "string",
                "format": "uri",
                "contentMediaType": "image/*"
              }
            ]
          }
        },
        "required": [
          "id",
          "created_at",
          "updated_at",
          "title",
          "body",
          "thumbnail"
        ]
      },
      "description": "Create a new article.\n\nWrites a new article and archives it into the DB.",
      "separated": {
        "llm": {
          "type": "object",
          "properties": {
            "input": {
              "description": "Information of the article to create.\n\n------------------------------\n\nDescription of the current {@link IBbsArticle.ICreate} type:\n\n> Information of the article to create.\n\n------------------------------\n\nDescription of the parent {@link IBbsArticle} type:\n\n> Article entity.\n> \n> `IBbsArticle` is an entity representing an article in the BBS (Bulletin Board System).",
              "type": "object",
              "properties": {
                "title": {
                  "title": "Title of the article",
                  "description": "Title of the article.\n\nRepresentative title of the article.",
                  "type": "string"
                },
                "body": {
                  "title": "Content body",
                  "description": "Content body.\n\nContent body of the article writtn in the markdown format.",
                  "type": "string"
                },
                "thumbnail": {
                  "title": "Thumbnail image URI",
                  "description": "Thumbnail image URI.\n\nThumbnail image URI which can represent the article.\n\nIf configured as `null`, it means that no thumbnail image in the article.",
                  "oneOf": [
                    {
                      "type": "null"
                    },
                    {
                      "type": "string",
                      "format": "uri",
                      "contentMediaType": "image/*"
                    }
                  ]
                }
              },
              "required": [
                "title",
                "body",
                "thumbnail"
              ]
            }
          },
          "required": [
            "input"
          ],
          "additionalProperties": false,
          "$defs": {}
        },
        "human": null
      }
    },
    {
      "name": "update",
      "parameters": {
        "type": "object",
        "properties": {
          "id": {
            "title": "Target article's {@link IBbsArticle.id}",
            "description": "Target article's {@link IBbsArticle.id}.",
            "type": "string",
            "format": "uuid"
          },
          "input": {
            "description": "Make all properties in T optional\n\n------------------------------\n\nDescription of the current {@link PartialIBbsArticle.ICreate} type:\n\n> Make all properties in T optional",
            "type": "object",
            "properties": {
              "title": {
                "title": "Title of the article",
                "description": "Title of the article.\n\nRepresentative title of the article.",
                "type": "string"
              },
              "body": {
                "title": "Content body",
                "description": "Content body.\n\nContent body of the article writtn in the markdown format.",
                "type": "string"
              },
              "thumbnail": {
                "title": "Thumbnail image URI",
                "description": "Thumbnail image URI.\n\nThumbnail image URI which can represent the article.\n\nIf configured as `null`, it means that no thumbnail image in the article.",
                "oneOf": [
                  {
                    "type": "null"
                  },
                  {
                    "type": "string",
                    "format": "uri",
                    "contentMediaType": "image/*"
                  }
                ]
              }
            },
            "required": [
              "title",
              "body",
              "thumbnail"
            ]
          }
        },
        "required": [
          "id",
          "input"
        ],
        "additionalProperties": false,
        "$defs": {}
      },
      "description": "Update an article.\n\nUpdates an article with new content.",
      "separated": {
        "llm": {
          "type": "object",
          "properties": {
            "id": {
              "title": "Target article's {@link IBbsArticle.id}",
              "description": "Target article's {@link IBbsArticle.id}.",
              "type": "string",
              "format": "uuid"
            },
            "input": {
              "description": "Make all properties in T optional\n\n------------------------------\n\nDescription of the current {@link PartialIBbsArticle.ICreate} type:\n\n> Make all properties in T optional",
              "type": "object",
              "properties": {
                "title": {
                  "title": "Title of the article",
                  "description": "Title of the article.\n\nRepresentative title of the article.",
                  "type": "string"
                },
                "body": {
                  "title": "Content body",
                  "description": "Content body.\n\nContent body of the article writtn in the markdown format.",
                  "type": "string"
                },
                "thumbnail": {
                  "title": "Thumbnail image URI",
                  "description": "Thumbnail image URI.\n\nThumbnail image URI which can represent the article.\n\nIf configured as `null`, it means that no thumbnail image in the article.",
                  "oneOf": [
                    {
                      "type": "null"
                    },
                    {
                      "type": "string",
                      "format": "uri",
                      "contentMediaType": "image/*"
                    }
                  ]
                }
              },
              "required": [
                "title",
                "body",
                "thumbnail"
              ]
            }
          },
          "required": [
            "id",
            "input"
          ],
          "additionalProperties": false,
          "$defs": {}
        },
        "human": null
      }
    },
    {
      "name": "erase",
      "parameters": {
        "type": "object",
        "properties": {
          "id": {
            "title": "Target article's {@link IBbsArticle.id}",
            "description": "Target article's {@link IBbsArticle.id}.",
            "type": "string",
            "format": "uuid"
          }
        },
        "required": [
          "id"
        ],
        "additionalProperties": false,
        "$defs": {}
      },
      "description": "Erase an article.\n\nErases an article from the DB.",
      "separated": {
        "llm": {
          "type": "object",
          "properties": {
            "id": {
              "title": "Target article's {@link IBbsArticle.id}",
              "description": "Target article's {@link IBbsArticle.id}.",
              "type": "string",
              "format": "uuid"
            }
          },
          "required": [
            "id"
          ],
          "additionalProperties": false,
          "$defs": {}
        },
        "human": null
      }
    }
  ]
}
```
  </Tabs.Tab>
</Tabs>




## Restrictions
`typia.llm.application<App, Model>()` follows the same restrictions of below.

About the function parameters type, it follows the restriction of both [`typia.llm.parameters<Params, Models>()`](./parameters) and [`typia.llm.schema<T, Model>()`](./schema) functions. Therefore, the parameters must be a keyworded object type with static keys without any dynamic keys. Also, the object type must not be nullable or optional.

About the return value type, it follows the restriction of [`typia.llm.schema<T, Model>()`](./schema) function. By the way, if the return type is union type with `undefined`, it would be compilation error, due to OpenAPI (JSON schema) specification does not support the undefindable union type.

  - [`typia.llm.parameters<Params, Models>()`](./parameters)
  - [`typia.llm.schema<T, Model>()`](./schema)

<Tabs items={["TypeScript Source Code", "Console Output"]}>
  <Tabs.Tab>
```typescript filename="src/examples/llm.application.violation.ts" showLineNumbers
import { ILlmApplication } from "@samchon/openapi";
import typia, { tags } from "typia";

const app: ILlmApplication<"chatgpt"> = typia.llm.application<
  BbsArticleController,
  "chatgpt"
>();

console.log(app);

interface BbsArticleController {
  /**
   * Create a new article.
   *
   * Writes a new article and archives it into the DB.
   *
   * @param props Properties of create function
   * @returns Newly created article
   */
  create(props: {
    /**
     * Information of the article to create
     */
    input: IBbsArticle.ICreate;
  }): Promise<IBbsArticle | undefined>;

  erase(id: string & tags.Format<"uuid">): Promise<void>;
}
```
  </Tabs.Tab>
  <Tabs.Tab>
```bash filename="Terminal"
src/examples/llm.application.violation.ts:4:41 - error TS(typia.llm.application): unsupported type detected    

- BbsArticleController.create: unknown
  - LLM application's function ("create")'s return type must not be union type with undefined.    

- BbsArticleController.erase: unknown
  - LLM application's function ("erase")'s parameter must be an object type.

4 const app: ILlmApplication<"chatgpt"> = typia.llm.application<
                                          ~~~~~~~~~~~~~~~~~~~~~~
5   BbsArticleController,
  ~~~~~~~~~~~~~~~~~~~~~~~
6   "chatgpt"
  ~~~~~~~~~~~
7 >();
  ~~~


Found 1 error in src/examples/llm.application.violation.ts:4
```
  </Tabs.Tab>
</Tabs>
