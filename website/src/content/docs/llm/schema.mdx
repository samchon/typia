---
title: Guide Documents > Large Language Model > schema() function
---
import { Callout, Tabs } from "nextra/components";

import LocalSource from "../../../components/LocalSource";
import RemoteSource from "../../../components/RemoteSource";

## `schema()` function

<Tabs items={[
    <code>typia</code>,
    <code>ILlmApplication</code>,
    <code>ILlmFunction</code>,
    <code>ILlmSchema</code>,
  ]}>
  <Tabs.Tab>
```typescript filename="typia" showLineNumbers {27-39}
export namespace llm {
  // LLM FUNCTION CALLING APPLICATION SCHEMA
  export function application<
    App extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    options?: Partial<Pick<ILlmApplication.IOptions<Model>, "separate">>,
  ): ILlmApplication<Model>;

  // STRUCTURED OUTPUT
  export function parameters<
    Parameters extends Record<string, any>,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(): ILlmSchema.ModelParameters[Model];

  // TYPE SCHEMA
  export function schema<
    T,
    Model extends ILlmSchema.Model,
    Config extends Partial<ILlmSchema.ModelConfig[Model]> = {},
  >(
    ...$defs: Extract<
      ILlmSchema.ModelSchema[Model],
      { $ref: string }
    > extends never
      ? []
      : [Record<string, ILlmSchema.ModelSchema[Model]>]
  ): ILlmSchema.ModelSchema[Model];
}
```
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/samchon/openapi/refs/heads/master/src/structures/ILlmApplication.ts"
      filename="@samchon/openapi"
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/samchon/openapi/refs/heads/master/src/structures/ILlmFunction.ts"
      filename="@samchon/openapi"
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <RemoteSource 
      url="https://raw.githubusercontent.com/samchon/openapi/refs/heads/master/src/structures/ILlmSchema.ts"
      filename="@samchon/openapi"
      showLineNumbers />
  </Tabs.Tab>
</Tabs>

Type schema in the LLM function calling application.

`typia.llm.schema<T, Model>()` is a function generating type schema which is used in the LLM (Large Language Model) function calling application schema or structured output, especially composed by the [`typia.llm.parameters<Parameters, Model>()`](./parameters) and [`typia.llm.application<App, Model>()`](./application) functions. 

Return value type `ILlmSchema` is similar with the JSON schema definition. However, its detailed specification becomes different by the LLM provider model you've chosen. Here is the list of LLM schema definitions of each model. Determine one of them carefully reading the LLM schema definitions.

  - Supported schemas
    - [`IChatGptSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IChatGptSchema.ts): OpenAI ChatGPT
    - [`IClaudeSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IClaudeSchema.ts): Anthropic Claude
    - [`IGeminiSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IGeminiSchema.ts): Google Gemini
    - [`ILlamaSchema`](https://github.com/samchon/openapi/blob/master/src/structures/ILlamaSchema.ts): Meta Llama
  - Midldle layer schemas
    - [`ILlmSchemaV3`](https://github.com/samchon/openapi/blob/master/src/structures/ILlmSchemaV3.ts): middle layer based on OpenAPI v3.0 specification
    - [`ILlmSchemaV3_1`](https://github.com/samchon/openapi/blob/master/src/structures/ILlmSchemaV3_1.ts): middle layer based on OpenAPI v3.1 specification

<Callout type="info">
**LLM Function Calling** and **Structured Output**

LLM selects proper function and fill arguments.

In nowadays, most LLM (Large Language Model) like OpenAI are supporting "function calling" feature. The "LLM function calling" means that LLM automatically selects a proper function and fills parameter values from conversation with the user (may by chatting text).

Structured output is another feature of LLM. The "structured output" means that LLM automatically transforms the output conversation into a structured data format like JSON.

- https://platform.openai.com/docs/guides/function-calling
- https://platform.openai.com/docs/guides/structured-outputs
</Callout>

<Tabs items={["TypeScript Source Code", "Compiled JavaScript"]}>
  <Tabs.Tab>
    <LocalSource 
      path="examples/src/llm/schema.ts"
      filename="example/src/llm/schema.ts"
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <LocalSource 
      path="examples/bin/llm/schema.js"
      filename="example/bin/llm/schema.js"
      showLineNumbers />
  </Tabs.Tab>
</Tabs>

## Specialization

You can utilize [type tags](../validators/tags/#type-tags) (or [validator's comment tags](../validators/tags/#comment-tags)) to constructing special fields of JSON schema. 

If you write any comment on a property, it would fill the `IJsonSchema.description` value. Also, there're special comment tags only for JSON schema definition that are different with [validator's comment tags](../validators/tags/#comment-tags) like below.

  - `@deprecated`
  - `@hidden`
  - `@internal`
  - `@title {string}`

Let's see how those [type tags](../validators/tags/#type-tags), comment tags and description comments are working with example code.

<Tabs items={['TypeScript Source Code', 'Compiled JavaScript File']}>
  <Tabs.Tab>
    <LocalSource 
      path="examples/src/llm/schema-special.ts"
      filename="example/src/llm/schema-special.ts"
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <LocalSource 
      path="examples/bin/llm/schema-special.js"
      filename="example/bin/llm/schema-special.js"
      showLineNumbers />
  </Tabs.Tab>
</Tabs>

## Customziation

If what you want is not just filling regular properties of LLM schema specification, but to adding custom properties into the JSON schema definition, you can do it through the `tags.TagBase.schema` property type or `tags.JsonSchemaPlugin` type. 

For reference, the custom property must be started with `x-` prefix. It's a rule of LLM schema.

<Tabs items={['TypeScript Source Code', 'Compiled JavaScript File']}>
  <Tabs.Tab>
    <LocalSource 
      path="examples/src/llm/schema-custom.ts"
      filename="example/src/llm/schema-custom.ts"
      showLineNumbers
      highlight="12-14, 18, 22-23" />
  </Tabs.Tab>
  <Tabs.Tab>
    <LocalSource 
      path="examples/bin/llm/schema-custom.js"
      filename="example/bin/llm/schema-custom.js"
      showLineNumbers
      highlight="7, 11" />
  </Tabs.Tab>
</Tabs>

## Restrictions

LLM schema does not support `bigint` type.

LLM schema is based on the JSON schema definition of the OpenAPI v3.0 specification. Therefore, limitations of the JSON schema is also applied to the LLM schema, and the `bigint` type is not supported in the LLM function calling schema composition.

Also, LLM schema does not support the tuple type, which is represented by the `OpenApi.IJsonSchema.ITuple` type. It's no LLM providers are supporting the tuple type, and such tuple type harms the separation option of the [`typia.llm.application<App, Model>()`](./application) function. If you need a tuple type, just change the tuple type to a regular object type instead.

<Tabs items={["TypeScript Source Code", "Console Output"]}>
  <Tabs.Tab>
```typescript filename="src/examples/llm.schema.bigint-and-tuple.ts" showLineNumbers
import typia from "typia";

typia.llm.schema<bigint, "chatgpt">({});
typia.llm.schema<[number, string], "claude">({});
```
  </Tabs.Tab>
  <Tabs.Tab>
```bash
src/examples/llm.schema.bigint-and-tuple.ts:3:1 - error TS(typia.llm.schema): unsupported type detected

- bigint
  - LLM schema does not support bigint type.

3 typia.llm.schema<bigint, "chatgpt">({});
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

src/examples/llm.schema.bigint-and-tuple.ts:4:1 - error TS(typia.llm.schema): failed to convert JSON schema to LLM schema.

  - $input.schema: LLM does not allow tuple type.

4 typia.llm.schema<[number, string], "claude">({});
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```
  </Tabs.Tab>
</Tabs>

If you're using Google Gemini ([`IGeminiSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IGeminiSchema.ts)) or middle layer schema of [`ILlmSchemaV3`](https://github.com/samchon/openapi/blob/master/src/structures/ILlmSchemaV3.ts), they do not support the reference type that is embodied by the `OpenApi.IJsonSchema.IReference` type with `$ref` property. Therefore, if recursive type comes, no way to express it perfectly in those [`IGeminiSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IGeminiSchema.ts) and [`ILlmSchemaV3`](https://github.com/samchon/openapi/blob/master/src/structures/ILlmSchemaV3.ts). They just repeat the recursive structure 3 times, and remove the recursive type after the 4 depths.

For reference, if the recursive type comes from the array type, it would be zero length array type at the fourth step. Otherwise the recursive type comes from a property and the property is optional, the 4th property would be removed from the object type. At last, if the recursive type is combined as an `oneOf` type, the type would be removed from there.

<Tabs items={["TypeScript Source Code", "Compiled JavaScript File"]}>
  <Tabs.Tab>
    <LocalSource 
      path="examples/src/llm/schema-recursive-gemini.ts"
      filename="example/src/llm/schema-recursive-gemini.ts"
      showLineNumbers />
  </Tabs.Tab>
  <Tabs.Tab>
    <LocalSource 
      path="examples/bin/llm/schema-recursive-gemini.js"
      filename="example/bin/llm/schema-recursive-gemini.js"
      showLineNumbers />
  </Tabs.Tab>
</Tabs>

And OpenAI ChatGPT ([`IChatGptSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IChatGptSchema.ts)) and Google Gemini ([`IGeminiSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IGeminiSchema.ts)) do not support the `OpenApi.IJsonSchema.IObject.additionalProperties` type, which represent the dynamic key typed object like `Record<string, T>` type in the TypeScript. Therefore, if you put the dynamic `Record<string, T>` type like below, `typia.llm.schema<T, "chatgpt">()` and `typia.llm.schema<T, "gemini">` functions throw the compilation error like below.

Therefore, if you want to utilzie OpenAI hatGPT ([`IChatGptSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IChatGptSchema.ts)) or Google Gemini ([`IGeminiSchema`](https://github.com/samchon/openapi/blob/master/src/structures/IGeminiSchema.ts)), you have to change the `Record<string, T>` type to an array of regular object type like below. Note that, as LLM providers do not support the `tuple` type, you don't have to define the array type containing the tuple type.

  - recommended: `Array<{ key: string, value: T }>`
  - not recommended: `Array<[string, T]>`

<Tabs items={["TypeScript Source Code", "Console Output"]}>
  <Tabs.Tab>
```typescript filename="src/examples/llm.schema.additionalProperties.ts" showLineNumbers
import typia from "typia";

typia.llm.schema<Record<string, number>, "chatgpt">({});
typia.llm.schema<Record<string, number>, "gemini">();
```
  </Tabs.Tab>
  <Tabs.Tab>
```bash
src/examples/llm.schema.additionalProperties.ts:3:1 - error TS(typia.llm.schema): unsupported type detected

- Recordstringnumber
  - LLM schema of "chatgpt" does not support dynamic property in object.  

- Recordstringnumber: Recordstringnumber
  - LLM schema of "chatgpt" does not support dynamic property in object.  

3 typia.llm.schema<Record<string, number>, "chatgpt">({});
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

src/examples/llm.schema.additionalProperties.ts:4:1 - error TS(typia.llm.schema): unsupported type detected

- Recordstringnumber
  - LLM schema of "gemini" does not support dynamic property in object.   

- Recordstringnumber: Recordstringnumber
  - LLM schema of "gemini" does not support dynamic property in object.   

4 typia.llm.schema<Record<string, number>, "gemini">();
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```
  </Tabs.Tab>
</Tabs>
